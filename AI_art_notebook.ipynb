{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82419d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#per vedere più immagini\n",
    "\n",
    "# Percorso alla cartella da cui vuoi prendere le 9 immagini\n",
    "img_dir = os.path.join(dataset_path, 'paired_dataset_art', 'damaged')\n",
    "\n",
    "# Prende 9 file casuali (assicurati che ci siano almeno 9 immagini!)\n",
    "img_files = random.sample(os.listdir(img_dir), 9)\n",
    "\n",
    "# Crea griglia 3x3\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Mostra le immagini\n",
    "for i, img_file in enumerate(img_files):\n",
    "    img_path = os.path.join(img_dir, img_file)\n",
    "    img = plt.imread(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"{img_file}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa047c",
   "metadata": {},
   "source": [
    "CODICE AGGIORNATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Librerie generali\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ Monta Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cef3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Percorsi base\n",
    "dataset_path = \"/content/drive/MyDrive/proj_cv/dataset\"\n",
    "paired_path = os.path.join(dataset_path, \"paired_dataset_art\")\n",
    "unpaired_path = os.path.join(dataset_path, \"unpaired_dataset_art\")\n",
    "damaged_path = os.path.join(paired_path, \"damaged\")\n",
    "undamaged_path = os.path.join(paired_path, \"undamaged\")\n",
    "\n",
    "# ✅ Percorso per immagini preprocessate\n",
    "output_dir = \"/content/drive/MyDrive/proj_cv/processed_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "processed_damaged_path = os.path.join(output_dir, \"damaged\")\n",
    "processed_undamaged_path = os.path.join(output_dir, \"undamaged\")\n",
    "\n",
    "os.makedirs(processed_damaged_path, exist_ok=True)\n",
    "os.makedirs(processed_undamaged_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ✅ Visualizza contenuto del dataset\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Visualizza una griglia di immagini casuali dal dataset\n",
    "categories = ['damaged', 'undamaged']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for ax_idx, dataset_type in enumerate([paired_path, unpaired_path]):\n",
    "    for i, category in enumerate(categories):\n",
    "        img_dir = os.path.join(dataset_type, category)\n",
    "        random_img = random.choice(os.listdir(img_dir))\n",
    "        img_path = os.path.join(img_dir, random_img)\n",
    "\n",
    "        img = plt.imread(img_path)\n",
    "        axes[ax_idx, i].imshow(img)\n",
    "        axes[ax_idx, i].set_title(f\"{dataset_type.split('/')[-1]} - {category}\")\n",
    "        axes[ax_idx, i].axis(\"off\")\n",
    "\n",
    "# Rimuove colonne vuote (3° colonna)\n",
    "if len(categories) < 3:\n",
    "    fig.delaxes(axes[0, 2])\n",
    "    fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Estrai 3 immagini danneggiate e cerca quelle restaurate\n",
    "damaged_images = os.listdir(damaged_path) # listdir ti ritorna la lista DEI NOMI delle immagini\n",
    "random_damaged = random.sample(damaged_images, 3)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "for i, damaged_img in enumerate(random_damaged):\n",
    "    damaged_img_path = os.path.join(damaged_path, damaged_img)\n",
    "    damaged_img_data = plt.imread(damaged_img_path)\n",
    "\n",
    "    # Costruisci nome restaurato\n",
    "    undamaged_img_name = damaged_img.replace('-before', '-after') #sto costruendo il nome delle immagini restuarate\n",
    "    undamaged_img_path = os.path.join(undamaged_path, undamaged_img_name)\n",
    "\n",
    "    if os.path.exists(undamaged_img_path):\n",
    "        undamaged_img_data = plt.imread(undamaged_img_path)\n",
    "    else:\n",
    "        undamaged_img_data = None\n",
    "\n",
    "    axes[i, 0].imshow(damaged_img_data)\n",
    "    axes[i, 0].set_title(f\"Damaged: {damaged_img}\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    if undamaged_img_data is not None:\n",
    "        axes[i, 1].imshow(undamaged_img_data)\n",
    "        axes[i, 1].set_title(f\"Restored: {undamaged_img_name}\")\n",
    "    else:\n",
    "        axes[i, 1].set_title(\"No matching restored image found\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Parametri e funzione di preprocessing\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\" Load, resize, normalize an image \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# ✅ Esegui preprocessing su una delle immagini danneggiate\n",
    "damaged_img_path = os.path.join(damaged_path, random_damaged[0])\n",
    "processed_img = preprocess_image(damaged_img_path)\n",
    "\n",
    "plt.imshow(processed_img)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define dataset paths\n",
    "paired_path = '/kaggle/input/damaged-and-undamaged-artworks/AI_for_Art_Restoration_2/paired_dataset_art'\n",
    "damaged_path = '/kaggle/input/damaged-and-undamaged-artworks/AI_for_Art_Restoration_2/paired_dataset_art/damaged'\n",
    "undamaged_path = '/kaggle/input/damaged-and-undamaged-artworks/AI_for_Art_Restoration_2/paired_dataset_art/undamaged'\n",
    "\n",
    "# Create output directories for processed images\n",
    "output_dir = \"/kaggle/working/processed_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"damaged\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"undamaged\"), exist_ok=True)\n",
    "\n",
    "# Get all images\n",
    "damaged_images = os.listdir(damaged_path)\n",
    "\n",
    "print(f\"Total Damaged Images: {len(damaged_images)}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\" Load, resize, denoise, and normalize an image \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Read in BGR format\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (256, 256))  # Resize to 256x256 for consistency\n",
    "\n",
    "    # Denoising: Apply Gaussian Blur\n",
    "    img = cv2.GaussianBlur(img, (5,5), 0)\n",
    "\n",
    "    # Normalize pixel values (0-1 range)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Process all images\n",
    "for damaged_img in damaged_images:\n",
    "    damaged_img_path = os.path.join(damaged_path, damaged_img)\n",
    "    processed_damaged = preprocess_image(damaged_img_path)\n",
    "\n",
    "    # Find corresponding undamaged image\n",
    "    undamaged_img_name = damaged_img.replace('-before', '-after')\n",
    "    undamaged_img_path = os.path.join(undamaged_path, undamaged_img_name)\n",
    "\n",
    "    if os.path.exists(undamaged_img_path):\n",
    "        processed_undamaged = preprocess_image(undamaged_img_path)\n",
    "        # Save the processed undamaged image\n",
    "        cv2.imwrite(os.path.join(output_dir, \"undamaged\", undamaged_img_name), (processed_undamaged * 255).astype(np.uint8))\n",
    "\n",
    "    # Save the processed damaged image\n",
    "    cv2.imwrite(os.path.join(output_dir, \"damaged\", damaged_img), (processed_damaged * 255).astype(np.uint8))\n",
    "\n",
    "print(\"✅ All images processed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ea400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3 random images for visualization\n",
    "sample_images = np.random.choice(damaged_images, 3, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    original_path = os.path.join(damaged_path, img_name)\n",
    "    processed_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "\n",
    "    # Load images\n",
    "    original_img = cv2.imread(original_path)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "    processed_img = cv2.imread(processed_path)\n",
    "    processed_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display images\n",
    "    axes[i, 0].imshow(original_img)\n",
    "    axes[i, 0].set_title(\"Original Damaged Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(processed_img)\n",
    "    axes[i, 1].set_title(\"Processed Image\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def apply_histogram_equalization(img):\n",
    "    # Convert image to grayscale and apply histogram equalization\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    equalized = cv2.equalizeHist(gray)  # Apply histogram equalization\n",
    "    return equalized\n",
    "\n",
    "# Select random sample images\n",
    "sample_images = np.random.choice(damaged_images, 3, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "    # Apply histogram equalization\n",
    "    equalized_img = apply_histogram_equalization(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(equalized_img, cmap='gray')  # Grayscale visualization\n",
    "    axes[i, 1].set_title(\"Histogram Equalized\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def apply_clahe(img):\n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    clahe_img = clahe.apply(gray)\n",
    "    return clahe_img\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe_img = apply_clahe(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(clahe_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"CLAHE Enhanced\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contrast_stretching(img):\n",
    "    \"\"\" Apply Min-Max Contrast Stretching \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    min_val, max_val = np.min(gray), np.max(gray)\n",
    "    stretched = ((gray - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "    return stretched\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply contrast stretching\n",
    "    stretched_img = apply_contrast_stretching(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(stretched_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Contrast Stretched\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def apply_contrast_stretching_rgb(img):\n",
    "    \"\"\" Apply Min-Max Contrast Stretching to each RGB channel \"\"\"\n",
    "    stretched = np.zeros_like(img)\n",
    "    for c in range(3):  # R, G, B\n",
    "        channel = img[:, :, c]\n",
    "        min_val, max_val = np.min(channel), np.max(channel)\n",
    "        if max_val > min_val:  # evita divisione per zero\n",
    "            stretched[:, :, c] = ((channel - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            stretched[:, :, c] = channel  # se tutti i pixel hanno lo stesso valore\n",
    "    return stretched\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Applica contrast stretching a colori\n",
    "    stretched_img = apply_contrast_stretching_rgb(img)\n",
    "\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(stretched_img)\n",
    "    axes[i, 1].set_title(\"Contrast Stretched (RGB)\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa92c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Preprocess + stretching\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_stretched = apply_contrast_stretching_rgb(img_rgb)\n",
    "\n",
    "# Convert to grayscale for Canny\n",
    "gray = cv2.cvtColor(img_stretched, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Apply Canny\n",
    "edges = cv2.Canny(gray, threshold1=50, threshold2=150)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Sample image path (puoi cambiarlo)\n",
    "img_name = sample_images[0]  # ad es. \"painting1-before.jpg\"\n",
    "img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "\n",
    "# Funzione: contrast stretching su canali RGB\n",
    "def apply_contrast_stretching_rgb(img):\n",
    "    stretched = np.zeros_like(img)\n",
    "    for c in range(3):  # R, G, B\n",
    "        channel = img[:, :, c]\n",
    "        min_val, max_val = np.min(channel), np.max(channel)\n",
    "        if max_val > min_val:\n",
    "            stretched[:, :, c] = ((channel - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            stretched[:, :, c] = channel\n",
    "    return stretched\n",
    "\n",
    "# STEP 1: Carica immagine originale\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# STEP 2: Applica contrast stretching\n",
    "img_stretched = apply_contrast_stretching_rgb(img_rgb)\n",
    "\n",
    "# STEP 3: Converti in grigio e applica Canny\n",
    "gray = cv2.cvtColor(img_stretched, cv2.COLOR_RGB2GRAY)\n",
    "edges = cv2.Canny(gray, threshold1=50, threshold2=150)\n",
    "\n",
    "# Visualizza tutto\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img_stretched)\n",
    "axes[1].set_title(\"Contrast Stretched (RGB)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(edges, cmap='gray')\n",
    "axes[2].set_title(\"Canny Edges\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21253394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sharpening(img):\n",
    "    \"\"\" Apply sharpening filter to enhance edges \"\"\"\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])  # Sharpening kernel\n",
    "    sharpened = cv2.filter2D(img, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "def apply_gaussian_smoothing(img):\n",
    "    \"\"\" Apply Gaussian blur to reduce noise \"\"\"\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    return blurred\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply filters\n",
    "    sharpened_img = apply_sharpening(img)\n",
    "    blurred_img = apply_gaussian_smoothing(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(sharpened_img)\n",
    "    axes[i, 1].set_title(\"Sharpened\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(blurred_img)\n",
    "    axes[i, 2].set_title(\"Gaussian Smoothed\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel(img):\n",
    "    \"\"\" Apply Sobel edge detection in both X and Y directions \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)  # Horizontal edges\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)  # Vertical edges\n",
    "    sobel_combined = cv2.magnitude(sobelx, sobely)  # Combine both\n",
    "    return sobel_combined\n",
    "\n",
    "def apply_canny(img):\n",
    "    \"\"\" Apply Canny edge detection \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)  # Tune thresholds as needed\n",
    "    return edges\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply edge detection\n",
    "    sobel_edges = apply_sobel(img)\n",
    "    canny_edges = apply_canny(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(sobel_edges, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Sobel Edge Detection\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(canny_edges, cmap='gray')\n",
    "    axes[i, 2].set_title(\"Canny Edge Detection\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholding(img):\n",
    "    \"\"\" Apply global thresholding \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)  # Global\n",
    "    return binary\n",
    "\n",
    "def apply_adaptive_thresholding(img):\n",
    "    \"\"\" Apply adaptive thresholding \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    return adaptive\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply thresholding\n",
    "    global_thresh = apply_thresholding(img)\n",
    "    adaptive_thresh = apply_adaptive_thresholding(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(global_thresh, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Global Thresholding\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(adaptive_thresh, cmap='gray')\n",
    "    axes[i, 2].set_title(\"Adaptive Thresholding\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NON SEGMENTA BENE\n",
    "def apply_watershed(img):\n",
    "    \"\"\" Apply Watershed Algorithm for segmentation \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Noise removal\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    # Sure background\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Finding sure foreground\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Marker labeling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all markers so that the unknown region is not 0\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    # Apply watershed\n",
    "    img_copy = img.copy()\n",
    "    markers = cv2.watershed(img_copy, markers)\n",
    "    img_copy[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n",
    "\n",
    "    return img_copy\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply Watershed\n",
    "    watershed_img = apply_watershed(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(watershed_img)\n",
    "    axes[i, 1].set_title(\"Watershed Segmented\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(img, k=3):\n",
    "    \"\"\" Apply K-Means clustering for segmentation \"\"\"\n",
    "    img_flat = img.reshape((-1, 3))  # Flatten the image\n",
    "    img_flat = np.float32(img_flat)\n",
    "\n",
    "    # Define criteria, number of clusters (K), and apply KMeans()\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    _, labels, centers = cv2.kmeans(img_flat, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Convert centers back to uint8 and map labels to segmented image\n",
    "    centers = np.uint8(centers)\n",
    "    segmented = centers[labels.flatten()]\n",
    "    segmented = segmented.reshape(img.shape)\n",
    "\n",
    "    return segmented\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply K-Means\n",
    "    kmeans_img = apply_kmeans(img, k=4)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(kmeans_img)\n",
    "    axes[i, 1].set_title(\"K-Means Segmented\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60be8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    \"\"\"Extract HOG features\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    features, hog_image = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return features, hog_image\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extract HOG features\n",
    "    hog_features, hog_img = extract_hog_features(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(hog_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"HOG Features\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def extract_lbp_features(img):\n",
    "    \"\"\"Extract LBP features\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    return lbp\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extract LBP features\n",
    "    lbp_img = extract_lbp_features(img)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(lbp_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"LBP Features\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#🧪 Confronta l’istogramma del dipinto danneggiato con quello restaurato per capire dove è cambiata la distribuzione cromatica.\n",
    "#si potranno mostrare tre grafici del color_histogram, uno per quello danneggiato, uno per la ground_truth e uno del modello\n",
    "\n",
    "def extract_color_histogram(img):\n",
    "    \"\"\"Extract color histograms for RGB channels\"\"\"\n",
    "    chans = cv2.split(img)\n",
    "    colors = (\"r\", \"g\", \"b\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    for chan, color in zip(chans, colors):\n",
    "        hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n",
    "        plt.plot(hist, color=color)\n",
    "\n",
    "    plt.title(\"Color Histogram\")\n",
    "    plt.xlabel(\"Bins\")\n",
    "    plt.ylabel(\"# of Pixels\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(output_dir, \"damaged\", img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Show images\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.subplot(3, 2, 2 * i + 2)\n",
    "    extract_color_histogram(img)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Image transformations (resize, normalize, tensor conversion)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to standard dimensions\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Custom Dataset class\n",
    "class ImageRestorationDataset(Dataset):\n",
    "    def __init__(self, damaged_dir, undamaged_dir, transform=None):\n",
    "        self.damaged_dir = damaged_dir\n",
    "        self.undamaged_dir = undamaged_dir\n",
    "        self.transform = transform\n",
    "        self.damaged_images = os.listdir(damaged_dir)\n",
    "        self.undamaged_images = set(os.listdir(undamaged_dir))  # Use a set for fast lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damaged_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damaged_img_name = self.damaged_images[idx]\n",
    "\n",
    "        # Find the corresponding undamaged image\n",
    "        base, ext = os.path.splitext(damaged_img_name)\n",
    "        undamaged_img_name = re.sub(r'([-_])before$', r'\\1after', base) + ext\n",
    "\n",
    "        undamaged_img_path = os.path.join(self.undamaged_dir, undamaged_img_name)\n",
    "\n",
    "        # Handle missing files\n",
    "        if undamaged_img_name not in self.undamaged_images:\n",
    "            print(f\"⚠️ Missing: {undamaged_img_name} → Skipping this sample.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Get another valid sample\n",
    "\n",
    "        # Load images\n",
    "        damaged_img_path = os.path.join(self.damaged_dir, damaged_img_name)\n",
    "        damaged_img = Image.open(damaged_img_path).convert(\"RGB\")\n",
    "        undamaged_img = Image.open(undamaged_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            damaged_img = self.transform(damaged_img)\n",
    "            undamaged_img = self.transform(undamaged_img)\n",
    "\n",
    "        return damaged_img, undamaged_img\n",
    "\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = ImageRestorationDataset(processed_damaged_path, processed_undamaged_path, transform)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 8  # Adjust based on GPU memory\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test DataLoader\n",
    "damaged_sample, undamaged_sample = next(iter(train_loader))\n",
    "print(f\"Batch shape (Damaged): {damaged_sample.shape}\")\n",
    "print(f\"Batch shape (Undamaged): {undamaged_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # 128x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # 32x32\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 128x128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # 256x256\n",
    "            nn.Tanh()  # Output values between -1 and 1 (normalized)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Move model to GPU\n",
    "model = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Mean Squared Error for pixel-wise loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Set number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for damaged_img, undamaged_img in train_loader:\n",
    "        damaged_img, undamaged_img = damaged_img.to(device), undamaged_img.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        restored_img = model(damaged_img)\n",
    "        loss = criterion(restored_img, undamaged_img)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fa una sorta di inferenza + valutazione ( da approfondire )\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for damaged_img, undamaged_img in dataloader:\n",
    "            damaged_img, undamaged_img = damaged_img.to(device), undamaged_img.to(device)\n",
    "\n",
    "            # Generate restored images\n",
    "            restored_img = model(damaged_img)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(restored_img, undamaged_img)\n",
    "            total_loss += loss.item() * damaged_img.size(0)\n",
    "            num_samples += damaged_img.size(0)\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    return avg_loss\n",
    "\n",
    "# Compute loss on test/validation set\n",
    "test_loss = evaluate_model(model, train_loader, criterion)\n",
    "print(f\"📉 Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 dB è accettabile\n",
    "#40+ dB è molto buona\n",
    "#50+ dB è praticamente perfetta\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = F.mse_loss(img1, img2).item()\n",
    "    if mse == 0:\n",
    "        return 100  # Perfect reconstruction\n",
    "    return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "\n",
    "def evaluate_psnr(model, dataloader):\n",
    "    model.eval()\n",
    "    total_psnr = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for damaged_img, undamaged_img in dataloader:\n",
    "            damaged_img, undamaged_img = damaged_img.to(device), undamaged_img.to(device)\n",
    "            restored_img = model(damaged_img)\n",
    "\n",
    "            # Compute PSNR\n",
    "            batch_psnr = sum(psnr(restored_img[i], undamaged_img[i]) for i in range(damaged_img.size(0)))\n",
    "            total_psnr += batch_psnr\n",
    "            num_samples += damaged_img.size(0)\n",
    "\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    return avg_psnr\n",
    "\n",
    "# Compute PSNR on test set\n",
    "psnr_score = evaluate_psnr(model, train_loader)\n",
    "print(f\"📊 PSNR Score: {psnr_score:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize image restoration\n",
    "def visualize_results(model, dataloader, num_images=5):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(10, 5 * num_images))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (damaged_img, undamaged_img) in enumerate(dataloader):\n",
    "            if i >= num_images:\n",
    "                break\n",
    "\n",
    "            # Move to device\n",
    "            damaged_img, undamaged_img = damaged_img.to(device), undamaged_img.to(device)\n",
    "            restored_img = model(damaged_img).cpu()\n",
    "\n",
    "            # Convert tensors to images\n",
    "            damaged_img = damaged_img.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            restored_img = restored_img.permute(0, 2, 3, 1).numpy()\n",
    "            undamaged_img = undamaged_img.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "            # Display results\n",
    "            axes[i, 0].imshow(damaged_img[0])\n",
    "            axes[i, 0].set_title(\"Damaged Image\")\n",
    "\n",
    "            axes[i, 1].imshow(restored_img[0])\n",
    "            axes[i, 1].set_title(\"Restored Image\")\n",
    "\n",
    "            axes[i, 2].imshow(undamaged_img[0])\n",
    "            axes[i, 2].set_title(\"Ground Truth\")\n",
    "\n",
    "            for ax in axes[i]:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "visualize_results(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
